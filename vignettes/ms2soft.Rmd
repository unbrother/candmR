---
title: "MS2Soft Functions Usage"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{MS2Soft Functions Usage}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(candmR)
```

# Introduction

Read the MS2Soft Webscraping Vignette before working with the `ms2_` functions.

The `MS2Soft` group of functions within the `candmR` package are designed to
gather traffic data from the MS2 available platforms by US State. 
The `platforms` dataset contains the current state of available platforms, to add
a new platform to the package, communicate to the author. To add a temporary 
platform, use the input_file.xlsx file.

```{r platforms}

platforms

```

# Input Data

The `attributes` dataset contains an example of an attributes list which can be 
imported by using an inputs file. An example inputs file is included with the
package in the external data directory. A simplified version can be created with
the `ms2_inputs` function, which will create an MS Excel file with the corresponding
inputs and export it into the working directory.
Similarly, the `stations` dataset contains an example of how stations should be
organized before starting any queries. The `ms2_stationsCols` function creates
an MS Excel file containing the required columns to perform the analysis. An 
explanation of the contents of each column can be found in the package 
documentation.

```{r ms2_inputs, eval = FALSE}

ms2_inputs()
ms2_stationsCols()

```

Once an attributes file has been created, it can be imported as a list with the 
`ms2_attributes` function, which takes a single argument containing the path to
the inputs_file.xlsx file. This step can be skipped by more advanced users if
all attributes are defined within R.

```{r ms2_attrs, eval = FALSE}

attributes <- ms2_attributes()

```

The attributes list has 7 elements, which are explained in detail in the
package documentation. The following chunk shows the contents of the `attributes`
dataset included as an example.

```{r print_ms2_attrs}

str(attributes)

```

In the same way, the stations list file can be imported as a dataframe using the
`ms2_readstations` function. Which depends on having a stations Excel file like
with the configuration shown in the example data, or the one created with the
`ms2_stationsCols`.

```{r ms2_stns, eval = FALSE}

stations_list <- ms2_readStations(path)

```

The structure of the `stations` data is shown next, as with the `attributes`, a
detailed description of each variable can be found in the package documentation.

```{r print_ms2_stns}

str(stations)

```

# Data Query

The data scraping is separated into two phases, one where users download the
available dates, and one where users download the actual traffic data.
To perform any analysis, users must gather a list of available dates, this helps 
the program avoid errors due to bad requests, and also saves time, as
not all date links are visitted during the data download phase.

## Gather Dates

The `ms2_all_dates` function takes the stations list and the attributes object
as arguments and returns a list of all available dates within the specified
date span, which is defined per station within the `stations` data (columns 
`start_date` and `end_date`). The `ms2_all_dates` function relies on the 
`ms2_dates` function, which gathers all available dates for only one station.

```{r get_dates, eval = FALSE}

dates_list <- ms2_all_dates(stations_list = stations_list, attributes = attributes)

```

The time to scrape one year of data per station is somewhat dependant on the
internet speed and the processing capabilities of the system, but it has been
timed around 5 minutes. Since this process consists of browsing along each day of
each year, regardless of the data being available or not, it is the most time 
consuming in terms of computer work, and because of this, the most prone to errors
due to network disconnection or power outages. The downloaded dates vector per station
will be stored in a TEMP folder, but the full list that serves as an input for the
next steps will only be created if the process is finished in full. 
Because of this, it is recommended to
split larger stations lists in sets, depending on how many years will be consulted.
As an example, if running 10 years of 6 stations (5 minutes x 10 years x 6 stations), 
users can expect their computers to be navigating during 5 hours, which is not 
very dangerous. On the other hand, having 5 years of 40 stations, would take 
almost 17 hours, same with 2 years of 80 stations, and so on. Users should
split their stations list in sets that allow for passive monitoring, like leaving
the computer working overnight (risky because internet providers usually do their
maintenance at this time, but practical) or splitting sets of 4 hours to be able
to check the results. If done on servers, larger stations sets can be consulted
at the same time, due to them not being as vulnerable from disconnections.
It is important to split the list by stations, not by years, so that all 
years are collected for each station in the same run.

When splitting, it is recommended to do a subset within R, and leave all stations
untouched in the stations_list file, this is useful because that file will serve as
a general record, as well as removing the need to have different input files or
having to modify it. 

To split the stations, users can do a simple subset in the stations_list object,
the following code performs the **ms2_all_dates** function over the first 10
stations of the stations_list object, then the next 10, while storing each result
in a dates_list numbered variable, which is then stored as the dates_list object
needed for the **ms2_all_data** function to run.

```{r split, eval = FALSE}

dates_list_1 <- ms2_all_dates(stations_list = stations_list[1:10,], attributes = attributes)
dates_list_2 <- ms2_all_dates(stations_list = stations_list[11:20,], attributes = attributes)

dates_list <- c(dates_list_1, dates_list_2)

```

From version 0.2.3, a TEMP folder is created containing all dates vectors created
during the downloads, as well as a log file that records which stations were
succesfully downloaded. This helps to not lose any progress from disconnections.
Running the **ms2_all_dates** function will can be performed as before, but the
process will create the mentioned folder, where each vector will be stored as 
an .RDS file. Using the attribute *gather_dates* allows for the function to 
create the dates_list from the files in the TEMP folder, instead of trying to 
download the data again. The *attributes* parameter is still needed to extract 
the analysis type (or using the analysis type parameter).

```{r gatherdates, eval = FALSE}

dates_list <- ms2_all_dates(attributes = attributes, gather_dates = TRUE)
dates_list <- ms2_all_dates(analysis_type = "perm", gather_dates = TRUE)

```

As of version 0.3.0, an additional function *ms2_fastdates()* was implemented,
allowing for faster downloads when treating short term and classification data 
(it can be run on permanent station but results are not useful, because it is known
in advance that the program will find many dates in the same year). 
The *ms2_fastdates()* function navigates to the front page of each station, and 
gathers the table displayed depending on the type of analysis to be performed.
Search terms have to be inputted as they would in the front page of the platform, 
currently it takes the district, the county and the community. This is used to
collect an in-session list that allows for the indexing of the stations within
the Selenium navigation session. These terms can be left blank, as this would not
have a significant impact in the processing time, but having the search terms kept
constant is useful for station identification along the process.


```{r fastdates, eval = FALSE}

# Gather dates for volume stations
dates_list <- ms2_fastdates(attributes, county = "Webb",
                          stations_list, table_type = "dates volume")

# Gather dates for classification station
dates_list <- ms2_fastdates(attributes, county = "Webb",
                          stations_list, table_type = "dates class")

```

It is worth noting that the *ms2_fastdates()* function is much faster than the
*ms2_all_dates()* function but can be less reliable as some dates might be 
reported as existing in the front page but in reality they are not.

Additionally, this function has the capacity of gathering the AADT table displayed
in this same front page. The gathered data is stored within a new folder created
inside the outputs folder and named yearly_volumes, as well as consolidated in a
table named aadt.

```{r aadt, eval = FALSE}

dates_list <- ms2_fastdates(attributes, county = "Webb",
                          stations_list, table_type = "aadt")

```

## Get Data

Next, use the `ms2_all_data` function to navigate to each of the URLs containing
the traffic data. Navigation is performed to reach each station, by given date 
(from the dates_list), by direction. And it's based on the analysis type defined
in the attributes. The function runs a Selenium session that navigates and 
downloads each file into the default downloads directory, and then copies them
into a "stations/" folder within the working directory. A subfolder referring to 
the analysis type is created, along with the corresponding years.

```{r get_data, eval = FALSE}

ms2_all_data(dates_list, stations_list = stations_list, attributes = attributes)

```

## Consolidate Database

Finally, the `ms2_createdb` function takes the attributes object as an argument,
as well as a class number (for class type analyses) to determine the 
reclassiffication table to be used. This function gathers all downloaded files
for that specific analysis type and consolidates a database containing all
values.

```{r database, eval = FALSE}

ms2_createdb(attributes)

```

